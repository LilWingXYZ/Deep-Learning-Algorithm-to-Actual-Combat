{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 语言模型与简单的WordEmbedding"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "什么是语言模型?\n",
    "\n",
    "语言模型刻画了一句话,它作为人话(语言),存在的概率"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "对于一句话\n",
    "$$sentence = w_1,w_2,\\cdots,w_n $$\n",
    "我们要求的概率为:\n",
    "$$\n",
    "P(sentence) = P( w_1,w_2,\\cdots,w_n)\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "按照概率公式展开，其形式如下：\n",
    "$$\n",
    "\\begin{array}{ll}\n",
    " P(sentence) & = P( w_1,w_2,\\cdots,w_n) \\\\\n",
    "            & = P(w_1) P(w_2 | w_1) P(w_3|w_2,w_1)  \\cdots P(w_n | w_1, \\cdots, w_{n-1})\n",
    "\\end{array}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "给定一个freestyle：\n",
    "\n",
    "> 嘿 嘿\n",
    "\n",
    "> 准备好了没有\n",
    "\n",
    "> 你看这个面它又长又宽\n",
    "\n",
    "> 就像这个碗它又大又圆\n",
    "\n",
    "> 你们 来这里 吃饭\n",
    "\n",
    "> 觉得 饭 很 好吃\n",
    "\n",
    "> 我看行\n",
    "\n",
    "> 你们 来这里 吃饭\n",
    "\n",
    "> 我给你们拉面一样很开心\n",
    "\n",
    "实际上，$P(freestyle)$是非常难算的"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\n",
    "\\begin{array}{ll}\n",
    " P(w_1)  = P(\\mbox{嘿})& = \\frac{\\#\\mbox{嘿}}{\\#\\mbox{总词数}} \\\\\n",
    " p(w_2 | w_1) = P(\\mbox{嘿} | \\mbox{嘿})           & = \\frac{\\#\\mbox{嘿,嘿}}{\\#(\\mbox{嘿},*)} \\\\\n",
    "  p(w_3 | w_1, w_2) = P(\\mbox{准备} | \\mbox{嘿},\\mbox{嘿})           & = \\frac{\\#\\mbox{嘿 嘿 准备}}{\\#(\\mbox{嘿,嘿},*)} \\\\\n",
    "    p(w_4 | w_1, w_2, w_3) = P(\\mbox{好} | \\mbox{嘿},\\mbox{嘿}，\\mbox{准备})           & = \\frac{\\#\\mbox{嘿 嘿 准备 好}}{\\#(\\mbox{嘿 嘿 准备},*)} \\\\\n",
    "    \\cdots \\\\\n",
    "\\end{array}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## N-gram 语言模型"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "为了简化$P(sentence)$， 人们强行设置了一个条件概率的依赖长度\n",
    "- 没有依赖的， 0-gram\n",
    "$$\n",
    "\\begin{array}{ll}\n",
    " P(sentence) & = P( w_1,w_2,\\cdots,w_n) \\\\\n",
    "            & = P(w_1) P(w_2 | w_1)  P(w_3|w_2,w_1)  \\cdots  P(w_n | w_1, \\cdots, w_{n-1}) \\\\\n",
    "            & \\approx P(w_1)  P(w_2)  \\cdots  P(w_n) \\\\\n",
    "            & = P(\\mbox{嘿})  P(\\mbox{嘿})  P(\\mbox{准备})  P(\\mbox{好})  \\cdots  P(\\mbox{开心})\n",
    "\\end{array}\n",
    "$$\n",
    "- 依赖为1的，就是1-gram\n",
    "$$\n",
    "\\begin{array}{ll}\n",
    " P(sentence) & = P( w_1,w_2,\\cdots,w_n) \\\\\n",
    "            & = P(w_1)  P(w_2 | w_1)  P(w_3|w_2,w_1)  \\cdots  P(w_n | w_1, \\cdots, w_{n-1}) \\\\\n",
    "            & \\approx P(w_1 | start)  P(w_2 | w_1)   P(w_3 | w_2)\\cdots  P(w_n|w_{n-1}) \\\\\n",
    "            & = P(\\mbox{嘿}| start)  P(\\mbox{嘿} | \\mbox{嘿})  P(\\mbox{准备} | \\mbox{嘿})  P(\\mbox{好} | \\mbox{准备})  \\cdots  P(\\mbox{开心} | \\mbox{很})\n",
    "\\end{array}\n",
    "$$\n",
    "- 依赖为2的，就是2-gram\n",
    "$$\n",
    " P(sentence) \\approx P(\\mbox{嘿}| start)  P(\\mbox{嘿} | \\mbox{嘿}, start)  P(\\mbox{准备} | \\mbox{嘿}, \\mbox{嘿})  P(\\mbox{好} | \\mbox{准备}, \\mbox{嘿})  \\cdots  P(\\mbox{开心} | \\mbox{很}, \\mbox{一样})\n",
    "$$\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "CONTEXT_SIZE = 2\n",
    "EMBEDDING_DIM = 10\n",
    "test_sentence = \"\"\" 嘿 嘿\n",
    "                    准备 好 了 没有\n",
    "                    你 看 这个 面 它 又 长 又 宽\n",
    "                    就 像 这个 碗 它 又 大 又 圆\n",
    "                    你们 来 这里 吃饭\n",
    "                    觉得 饭 很 好吃\n",
    "                    我 看 行\n",
    "                    你们 来 这里 吃饭\n",
    "                    我 给 你们 拉 面 一样 很 开心\"\"\".split()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(['嘿', '嘿'], '准备'), (['嘿', '准备'], '好'), (['准备', '好'], '了'), (['好', '了'], '没有'), (['了', '没有'], '你'), (['没有', '你'], '看'), (['你', '看'], '这个'), (['看', '这个'], '面'), (['这个', '面'], '它'), (['面', '它'], '又')]\n"
     ]
    }
   ],
   "source": [
    "trigrams = [([test_sentence[i], test_sentence[i + 1]], test_sentence[i + 2]) for i in range(len(test_sentence) - 2)]\n",
    "print(trigrams[:10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'你', '它', '圆', '宽', '看', '就', '觉得', '你们', '好吃', '一样', '嘿', '准备', '这里', '好', '很', '来', '给', '我', '面', '又', '开心', '拉', '吃饭', '了', '长', '碗', '没有', '行', '这个', '大', '像', '饭'}\n"
     ]
    }
   ],
   "source": [
    "vocab = set(test_sentence)\n",
    "print(vocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'你': 0, '它': 1, '圆': 2, '宽': 3, '看': 4, '就': 5, '觉得': 6, '你们': 7, '好吃': 8, '一样': 9, '嘿': 10, '准备': 11, '这里': 12, '好': 13, '很': 14, '来': 15, '给': 16, '我': 17, '面': 18, '又': 19, '开心': 20, '拉': 21, '吃饭': 22, '了': 23, '长': 24, '碗': 25, '没有': 26, '行': 27, '这个': 28, '大': 29, '像': 30, '饭': 31}\n"
     ]
    }
   ],
   "source": [
    "word_to_id = {word: i for i, word in enumerate(vocab)}\n",
    "id_to_word = {i: word for i, word in enumerate(vocab)}\n",
    "print(word_to_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$ p(w_n | w_{n-1}, w_{n+1}) $$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "class NgramLanguageModel(nn.Module):\n",
    "\n",
    "    def __init__(self, vocab_size, embedding_dim, context_size):\n",
    "        super(NgramLanguageModel, self).__init__()\n",
    "        self.embeddings = nn.Embedding(vocab_size, embedding_dim)\n",
    "        self.linear1 = nn.Linear(context_size * embedding_dim, 128)\n",
    "        self.linear2 = nn.Linear(128, vocab_size)\n",
    "\n",
    "    def forward(self, inputs):\n",
    "        embeds = self.embeddings(inputs).view((1, -1))\n",
    "        out = self.linear1(embeds)\n",
    "        out = F.relu(out)\n",
    "        out = self.linear2(out)\n",
    "        log_probs = F.log_softmax(out, dim=1)\n",
    "        return log_probs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss_function = nn.NLLLoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = NgramLanguageModel(len(vocab), EMBEDDING_DIM, CONTEXT_SIZE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = optim.SGD(model.parameters(), lr=0.001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(['嘿', '嘿'], '准备'),\n",
       " (['嘿', '准备'], '好'),\n",
       " (['准备', '好'], '了'),\n",
       " (['好', '了'], '没有'),\n",
       " (['了', '没有'], '你'),\n",
       " (['没有', '你'], '看'),\n",
       " (['你', '看'], '这个'),\n",
       " (['看', '这个'], '面'),\n",
       " (['这个', '面'], '它'),\n",
       " (['面', '它'], '又'),\n",
       " (['它', '又'], '长'),\n",
       " (['又', '长'], '又'),\n",
       " (['长', '又'], '宽'),\n",
       " (['又', '宽'], '就'),\n",
       " (['宽', '就'], '像'),\n",
       " (['就', '像'], '这个'),\n",
       " (['像', '这个'], '碗'),\n",
       " (['这个', '碗'], '它'),\n",
       " (['碗', '它'], '又'),\n",
       " (['它', '又'], '大'),\n",
       " (['又', '大'], '又'),\n",
       " (['大', '又'], '圆'),\n",
       " (['又', '圆'], '你们'),\n",
       " (['圆', '你们'], '来'),\n",
       " (['你们', '来'], '这里'),\n",
       " (['来', '这里'], '吃饭'),\n",
       " (['这里', '吃饭'], '觉得'),\n",
       " (['吃饭', '觉得'], '饭'),\n",
       " (['觉得', '饭'], '很'),\n",
       " (['饭', '很'], '好吃'),\n",
       " (['很', '好吃'], '我'),\n",
       " (['好吃', '我'], '看'),\n",
       " (['我', '看'], '行'),\n",
       " (['看', '行'], '你们'),\n",
       " (['行', '你们'], '来'),\n",
       " (['你们', '来'], '这里'),\n",
       " (['来', '这里'], '吃饭'),\n",
       " (['这里', '吃饭'], '我'),\n",
       " (['吃饭', '我'], '给'),\n",
       " (['我', '给'], '你们'),\n",
       " (['给', '你们'], '拉'),\n",
       " (['你们', '拉'], '面'),\n",
       " (['拉', '面'], '一样'),\n",
       " (['面', '一样'], '很'),\n",
       " (['一样', '很'], '开心')]"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trigrams"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4.896181091666222\n",
      "4.323531731963158\n",
      "3.815610770136118\n",
      "3.3711194321513176\n",
      "2.979199256747961\n",
      "2.621775332838297\n",
      "2.2896604537963867\n",
      "1.981300052255392\n",
      "1.7002220638096333\n",
      "1.4492377378046513\n",
      "1.2299880422651768\n",
      "1.0424759909510612\n",
      "0.885625995695591\n",
      "0.7568334415555\n",
      "0.6524491757154465\n",
      "0.5683880597352982\n",
      "0.5009026676416397\n",
      "0.4466545283794403\n",
      "0.40271973609924316\n",
      "0.36678197979927063\n",
      "0.3370480537414551\n",
      "0.31222742795944214\n",
      "0.2912861406803131\n",
      "0.2734541594982147\n",
      "0.2581346482038498\n",
      "0.24486128985881805\n",
      "0.23327478766441345\n",
      "0.22309532761573792\n",
      "0.21409006416797638\n",
      "0.20608000457286835\n",
      "0.19892030954360962\n",
      "0.19248811900615692\n",
      "0.18667468428611755\n",
      "0.18141070008277893\n",
      "0.17662087082862854\n",
      "0.17225471138954163\n",
      "0.16825039684772491\n",
      "0.16457854211330414\n",
      "0.16119390726089478\n",
      "0.15806061029434204\n",
      "0.15516133606433868\n",
      "0.15247492492198944\n",
      "0.14998182654380798\n",
      "0.1476566195487976\n",
      "0.1454886645078659\n",
      "0.14345882833003998\n",
      "0.14156000316143036\n",
      "0.1397707313299179\n",
      "0.1380997747182846\n",
      "0.13651655614376068\n"
     ]
    }
   ],
   "source": [
    "for epoch in range(1000):\n",
    "    total_loss = 0\n",
    "    for context, target in trigrams:\n",
    "        context_tensor = torch.tensor([word_to_id[w] for w in context], dtype=torch.long)\n",
    "        optimizer.zero_grad()\n",
    "        log_probs = model(context_tensor)\n",
    "        loss = loss_function(log_probs, torch.tensor([word_to_id[target]], dtype=torch.long))\n",
    "\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        total_loss += loss.item()\n",
    "    if epoch % 20 == 0:\n",
    "        print(total_loss/len(vocab))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_sentence = \"\"\" 嘿 嘿\n",
    "                    准备 好 了 没有\n",
    "                    你 看 这个 面 它 又 长 又 宽\n",
    "                    就 像 这个 碗 它 又 大 又 圆\n",
    "                    你们 来 这里 吃饭\n",
    "                    觉得 饭 很 好吃\n",
    "                    我 看 行\n",
    "                    你们 来 这里 吃饭\n",
    "                    我 给 你们 拉 面 一样 很 开心\"\"\".split()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'开心'"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data = model(torch.tensor([word_to_id[\"一样\"],word_to_id[\"很\"]]))\n",
    "log_prob, index = data.max(1)\n",
    "id_to_word[index.item()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([-0.9184,  0.3898, -1.5934,  1.3574, -1.2248,  0.0207, -1.2754,  0.6992,\n",
       "         0.5606,  0.3963], grad_fn=<EmbeddingBackward>)"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a = model.embeddings(torch.tensor(word_to_id[\"你们\"]))\n",
    "a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([ 1.3453, -0.4587, -0.4180,  0.2134, -0.2366, -0.5281,  0.2433, -1.5369,\n",
       "        -0.2175, -0.1851], grad_fn=<EmbeddingBackward>)"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "b = model.embeddings(torch.tensor(word_to_id[\"你\"]))\n",
    "b"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
